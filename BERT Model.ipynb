{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jj0LPMF13xl4",
    "outputId": "cf674505-7523-4f09-a06d-e6c40c88c45e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in c:\\users\\sanke\\anaconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\sanke\\anaconda3\\lib\\site-packages (from tensorflow_hub) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\sanke\\anaconda3\\lib\\site-packages (from tensorflow_hub) (1.20.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U tensorflow-text\n",
    "!pip install -q tf-models-official\n",
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M00HMxyi3ASB",
    "outputId": "7123a371-5f97-4d3b-dad7-46c7ff09746a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sanke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sanke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "import wordcloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras import backend as K\n",
    "from transformers import AutoTokenizer,TFBertModel\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, BinaryAccuracy\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy,BinaryCrossentropy\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "QRm2Lhja3kDG",
    "outputId": "667a954f-644c-40cc-88b4-7a08a9dc6eb7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  target\n",
       "0   1  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1   4             Forest fire near La Ronge Sask. Canada       1\n",
       "2   5  All residents asked to 'shelter in place' are ...       1\n",
       "3   6  13,000 people receive #wildfires evacuation or...       1\n",
       "4   7  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop(['keyword','location'], axis = 1)\n",
    "test = test.drop(['keyword','location'], axis = 1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train set: (7613, 3)\n",
      "Shape of Test set: (3263, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checking Shape of Train and Test sets:\n",
    "print(\"Shape of Train set:\", train.shape)\n",
    "print(\"Shape of Test set:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gw2tIJ_C3i-C"
   },
   "source": [
    "# Labels are as follows:\n",
    "label '1' ---> racist/sexist tweet           \n",
    "label '0' ---> not racist/sexist tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fleUSdtI3kOZ",
    "outputId": "a8196139-0e5d-4bf4-e691-0de9b672c36a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train.copy()\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R338ymKtGmI8"
   },
   "source": [
    "# 1. Model without removing any feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4DR_rqRGyOT"
   },
   "source": [
    "### Splitting data into Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eXZiaKYnGuAp"
   },
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(df['target'], num_classes=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV4EGmipG-1y"
   },
   "source": [
    "# BERT \n",
    "### Base Model with Neural Networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_2eseBvNGt8U"
   },
   "outputs": [],
   "source": [
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\")\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WhbITOFaGt6C",
    "outputId": "3e3e40b0-7908-404a-9cdc-4ec5d3da36bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1024), dtype=float32, numpy=\n",
       "array([[ 0.99035466,  0.9815679 ,  0.9975458 , ..., -0.9994887 ,\n",
       "        -0.5053125 ,  0.9433913 ],\n",
       "       [ 0.9989152 ,  0.11918571,  0.8990445 , ..., -0.63001037,\n",
       "        -0.9233927 ,  0.9344142 ]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking array created using BERT:\n",
    "def get_sentence_embedding(sentences):\n",
    "  preprocessed_text = bert_preprocess(sentences)\n",
    "  return bert_encoder(preprocessed_text)['pooled_output']\n",
    "\n",
    "get_sentence_embedding([\"You are noob.\",\"What are you looking at?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdQ0GPmbGt25",
    "outputId": "553c8161-2566-4907-c334-4264cf135750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       {'input_mask': (Non  0           ['text[0][0]']                   \n",
      "                                e, 128),                                                          \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " keras_layer_1 (KerasLayer)     {'default': (None,   333579265   ['keras_layer[0][0]',            \n",
      "                                1024),                            'keras_layer[0][1]',            \n",
      "                                 'pooled_output': (               'keras_layer[0][2]']            \n",
      "                                None, 1024),                                                      \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, 128, 1024),                                               \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                , (None, 128, 1024)                                               \n",
      "                                ]}                                                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['keras_layer_1[0][25]']         \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 2)            2050        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 333,581,315\n",
      "Trainable params: 2,050\n",
      "Non-trainable params: 333,579,265\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Bert layers:\n",
    "num_classes = 2\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "# Neural network layers:\n",
    "l = tf.keras.layers.Dropout(0.2, name='dropout')(outputs['pooled_output'])\n",
    "l = tf.keras.layers.Dense(num_classes, activation='sigmoid', name='output')(l)\n",
    "\n",
    "# Construct final model:\n",
    "model = tf.keras.Model(inputs=[text_input], outputs=[l])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "METRICS = [\n",
    "           tf.keras.metrics.BinaryCrossentropy(name='accuracy'),\n",
    "           tf.keras.metrics.Precision(name='precision'),\n",
    "           tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "SMh9FauXHV8-",
    "outputId": "0da8485c-12c4-4f79-fe1b-7d8f9fc9f076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "#Ploting Model Architecture:\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DRYvpP9JCXh"
   },
   "source": [
    "### Training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GtjGaPN9HV__",
    "outputId": "9f1e6814-f109-4581-9ba7-a3d72acdeb75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "191/191 [==============================] - 224s 1s/step - loss: 0.7952 - accuracy: 0.7322 - precision: 0.5399 - recall: 0.5415\n",
      "Epoch 2/4\n",
      "191/191 [==============================] - 228s 1s/step - loss: 0.7054 - accuracy: 0.6915 - precision: 0.5772 - recall: 0.5791\n",
      "Epoch 3/4\n",
      "191/191 [==============================] - 230s 1s/step - loss: 0.6584 - accuracy: 0.6682 - precision: 0.5978 - recall: 0.5947\n",
      "Epoch 4/4\n",
      "191/191 [==============================] - 228s 1s/step - loss: 0.6355 - accuracy: 0.6568 - precision: 0.6170 - recall: 0.6167\n",
      "48/48 [==============================] - 58s 1s/step - loss: 0.5632 - accuracy: 0.6108 - precision: 0.7018 - recall: 0.7341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5631595849990845, 0.610754668712616, 0.7018204927444458, 0.7340774536132812]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=4)\n",
    "\n",
    "# Evaluating results with test set:\n",
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4EBwwBsHbHo"
   },
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B7D6dPi0HWFL",
    "outputId": "869001b7-0bd5-4c7a-de52-698964569dc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[602 239]\n",
      " [187 495]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       841\n",
      "           1       0.67      0.73      0.70       682\n",
      "\n",
      "    accuracy                           0.72      1523\n",
      "   macro avg       0.72      0.72      0.72      1523\n",
      "weighted avg       0.72      0.72      0.72      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_arg = np.argmax(y_test, axis=1)\n",
    "y_test_arg[1]\n",
    "y_pred = np.argmax(model.predict(X_test),axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test_arg, y_pred))\n",
    "print(metrics.classification_report(y_test_arg, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yz9aU8MRIjZe"
   },
   "source": [
    "# 2. Model after removing stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "judFCs9VIm9-"
   },
   "outputs": [],
   "source": [
    "df = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xm7rcox7IsaH"
   },
   "source": [
    "### Removing Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XVw75OFyIwxo"
   },
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_767Q4kPI28l"
   },
   "source": [
    "### Splitting data into Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Rgho-xB7Iwvc"
   },
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(df['target'], num_classes=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXwzufACI_Q8"
   },
   "source": [
    "### Training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "855Z6G95IwqY",
    "outputId": "24bb5d07-c3a8-4f15-f2e8-0a580a72aea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "191/191 [==============================] - 237s 1s/step - loss: 0.6349 - accuracy: 0.6512 - precision: 0.6238 - recall: 0.6312\n",
      "Epoch 2/3\n",
      "191/191 [==============================] - 241s 1s/step - loss: 0.6104 - accuracy: 0.6369 - precision: 0.6404 - recall: 0.6450\n",
      "Epoch 3/3\n",
      "191/191 [==============================] - 236s 1s/step - loss: 0.5973 - accuracy: 0.6308 - precision: 0.6508 - recall: 0.6471\n",
      "48/48 [==============================] - 78s 2s/step - loss: 0.5318 - accuracy: 0.5857 - precision: 0.7383 - recall: 0.7741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5318173170089722,\n",
       " 0.5857071876525879,\n",
       " 0.7382592558860779,\n",
       " 0.7741299867630005]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=3)\n",
    "\n",
    "# Evaluating results with test set:\n",
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-b02s4aJRjb"
   },
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UkY86LqKIwoP",
    "outputId": "a7cbffc5-f7eb-4c41-fb2e-53f3cf719cd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[697 144]\n",
      " [230 452]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.83      0.79       841\n",
      "           1       0.76      0.66      0.71       682\n",
      "\n",
      "    accuracy                           0.75      1523\n",
      "   macro avg       0.76      0.75      0.75      1523\n",
      "weighted avg       0.75      0.75      0.75      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_arg = np.argmax(y_test, axis=1)\n",
    "y_test_arg[1]\n",
    "y_pred = np.argmax(model.predict(X_test),axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test_arg, y_pred))\n",
    "print(metrics.classification_report(y_test_arg, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dELBCFu8JUwd"
   },
   "source": [
    "# 3. Model after removing repeating characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "5VT6YGXpJdiM"
   },
   "outputs": [],
   "source": [
    "df = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEjPIcV7JUs_"
   },
   "source": [
    "### Removing repeating characteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4A9TEHapIwmE"
   },
   "outputs": [],
   "source": [
    "tokens = (word_tokenize(i) for i in df.text)\n",
    "df['text'] = df['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "pattern = re.compile(r'(.)\\1*')\n",
    "\n",
    "def reduce_sequence_word(word):\n",
    "    return ''.join([match.group()[:2] if len(match.group()) > 2 else match.group() for match in pattern.finditer(word)])\n",
    "\n",
    "def reduce_sequence_tweet(tweet):\n",
    "    return [reduce_sequence_word(word) for word in tweet]\n",
    "\n",
    "df.text = df.text.apply(lambda tweet: reduce_sequence_tweet(tweet))\n",
    "\n",
    "# Detokenizing tweets:\n",
    "\n",
    "def listToString(s): \n",
    "    \n",
    "    # initialize an empty string\n",
    "    str1 = \" \" \n",
    "    \n",
    "    # return string  \n",
    "    return (str1.join(s))\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: listToString(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRU61clEJnO7"
   },
   "source": [
    "### Splitting data into Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1Ap7UDZfIwhz"
   },
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(df['target'], num_classes=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRSOFwRgJuHe"
   },
   "source": [
    "### Training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zfXgZs8TJq8D",
    "outputId": "2fd6a561-edfa-452c-85a1-5a048354b03c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "191/191 [==============================] - 2085s 11s/step - loss: 0.5889 - accuracy: 0.6262 - precision: 0.6553 - recall: 0.6524\n",
      "Epoch 2/3\n",
      "191/191 [==============================] - 26963s 142s/step - loss: 0.5902 - accuracy: 0.6247 - precision: 0.6534 - recall: 0.6509\n",
      "Epoch 3/3\n",
      "191/191 [==============================] - 206s 1s/step - loss: 0.5928 - accuracy: 0.6241 - precision: 0.6523 - recall: 0.6448\n",
      "48/48 [==============================] - 256s 5s/step - loss: 0.5394 - accuracy: 0.5914 - precision: 0.7183 - recall: 0.7433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5394353866577148,\n",
       " 0.5913888812065125,\n",
       " 0.7182741165161133,\n",
       " 0.7432698607444763]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=3)\n",
    "\n",
    "# Evaluating results with test set:\n",
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QRues6VJwkf"
   },
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pvwj-Tm7Jq-c",
    "outputId": "a6db0f71-a7c3-4269-bc32-c4ef0642911f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[613 228]\n",
      " [179 503]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75       841\n",
      "           1       0.69      0.74      0.71       682\n",
      "\n",
      "    accuracy                           0.73      1523\n",
      "   macro avg       0.73      0.73      0.73      1523\n",
      "weighted avg       0.74      0.73      0.73      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_arg = np.argmax(y_test, axis=1)\n",
    "y_test_arg[1]\n",
    "y_pred = np.argmax(model.predict(X_test),axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test_arg, y_pred))\n",
    "print(metrics.classification_report(y_test_arg, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXECfDyLJzT7"
   },
   "source": [
    "# 4. Model after removing Punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "qK0EU7EeJrAl"
   },
   "outputs": [],
   "source": [
    "df = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CszDScGqJ7Y1"
   },
   "source": [
    "### Removing Punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xxs9pUT0JrCu",
    "outputId": "56018a58-b9ed-4b14-9c0f-6f943ca8d75b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "aH2u3qqgJrFT"
   },
   "outputs": [],
   "source": [
    "punctuations_list = string.punctuation\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: cleaning_punctuations(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGLTnY9NJ-P2"
   },
   "source": [
    "### Splitting data into Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NaHjtYwCJrHv"
   },
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(df['target'], num_classes=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OK1yotwKUu8"
   },
   "source": [
    "### Training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8TP1D-dJrMC",
    "outputId": "7a607ece-fcb4-40ce-c14e-0fdd1b0452da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "191/191 [==============================] - 304s 2s/step - loss: 0.5893 - accuracy: 0.6268 - precision: 0.6579 - recall: 0.6195\n",
      "Epoch 2/3\n",
      "191/191 [==============================] - 243s 1s/step - loss: 0.5797 - accuracy: 0.6202 - precision: 0.6696 - recall: 0.6282\n",
      "Epoch 3/3\n",
      "191/191 [==============================] - 2571s 14s/step - loss: 0.5799 - accuracy: 0.6148 - precision: 0.6751 - recall: 0.6361\n",
      "48/48 [==============================] - 52s 1s/step - loss: 0.5291 - accuracy: 0.5861 - precision: 0.7641 - recall: 0.7360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5291005969047546,\n",
       " 0.5861385464668274,\n",
       " 0.7641445398330688,\n",
       " 0.7360472679138184]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=3)\n",
    "\n",
    "# Evaluating results with test set:\n",
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLluY7b5Kait"
   },
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0253zglJrOG",
    "outputId": "6b408487-cc86-4e1d-dcc2-f6e7b88ff74d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[748  93]\n",
      " [260 422]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.89      0.81       841\n",
      "           1       0.82      0.62      0.71       682\n",
      "\n",
      "    accuracy                           0.77      1523\n",
      "   macro avg       0.78      0.75      0.76      1523\n",
      "weighted avg       0.78      0.77      0.76      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_arg = np.argmax(y_test, axis=1)\n",
    "y_test_arg[1]\n",
    "y_pred = np.argmax(model.predict(X_test),axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test_arg, y_pred))\n",
    "print(metrics.classification_report(y_test_arg, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvMEqQdHKdDg"
   },
   "source": [
    "# 5. Model after removing numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "23VpbPYSKid3"
   },
   "outputs": [],
   "source": [
    "df = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6F8QjQTKc84"
   },
   "source": [
    "### Removing numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "QtMPakeBKoAF"
   },
   "outputs": [],
   "source": [
    "def cleaning_numbers(text):\n",
    "    return re.sub('[0-9]+', '', text)\n",
    "\n",
    "df['text'] = df['text'].apply(lambda text: cleaning_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYzzt9PyKp0Z"
   },
   "source": [
    "### Splitting data into Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "M0aJGiFwKvjM"
   },
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(df['target'], num_classes=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYVbwsseK5GE"
   },
   "source": [
    "### Training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_wVKVmmKvno",
    "outputId": "ab2ccaae-cd3b-4a2d-e8ba-a616bafb31f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "191/191 [==============================] - 220s 1s/step - loss: 0.5822 - accuracy: 0.6191 - precision: 0.6593 - recall: 0.6537\n",
      "Epoch 2/3\n",
      "191/191 [==============================] - 228s 1s/step - loss: 0.6028 - accuracy: 0.6273 - precision: 0.6521 - recall: 0.6580\n",
      "Epoch 3/3\n",
      "191/191 [==============================] - 231s 1s/step - loss: 0.6022 - accuracy: 0.6292 - precision: 0.6474 - recall: 0.6573\n",
      "48/48 [==============================] - 58s 1s/step - loss: 0.5324 - accuracy: 0.5822 - precision: 0.7500 - recall: 0.7702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5323783159255981, 0.5822197794914246, 0.75, 0.770190417766571]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=3)\n",
    "\n",
    "# Evaluating results with test set:\n",
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0USOcWEgK6IP"
   },
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15Vmft4CKvra",
    "outputId": "fc3ba2b6-9e49-4da7-cda3-b76f4e55a71a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[749  92]\n",
      " [272 410]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.89      0.80       841\n",
      "           1       0.82      0.60      0.69       682\n",
      "\n",
      "    accuracy                           0.76      1523\n",
      "   macro avg       0.78      0.75      0.75      1523\n",
      "weighted avg       0.77      0.76      0.75      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_arg = np.argmax(y_test, axis=1)\n",
    "y_test_arg[1]\n",
    "y_pred = np.argmax(model.predict(X_test),axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test_arg, y_pred))\n",
    "print(metrics.classification_report(y_test_arg, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup 5: Applying Stemming and Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Stemming: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing tweets:\n",
    "tokens = (word_tokenize(i) for i in df.text)\n",
    "df['text'] = df['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "stemm = SnowballStemmer('english')\n",
    "df['text'] = df['text'].apply(lambda x: [stemm.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into Train and Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(df['target'].astype(str), num_classes=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'].astype(str), y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "191/191 [==============================] - 237s 1s/step - loss: 0.6885 - accuracy: 0.7190 - precision: 0.6102 - recall: 0.3315\n",
      "Epoch 2/4\n",
      "191/191 [==============================] - 240s 1s/step - loss: 0.6527 - accuracy: 0.7033 - precision: 0.6434 - recall: 0.3466\n",
      "Epoch 3/4\n",
      "191/191 [==============================] - 241s 1s/step - loss: 0.6536 - accuracy: 0.7008 - precision: 0.6473 - recall: 0.3598\n",
      "Epoch 4/4\n",
      "191/191 [==============================] - 242s 1s/step - loss: 0.6405 - accuracy: 0.6952 - precision: 0.6623 - recall: 0.3675\n",
      "48/48 [==============================] - 61s 1s/step - loss: 0.6004 - accuracy: 0.6659 - precision: 0.7326 - recall: 0.3040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6003526449203491, 0.6659488081932068, 0.732594907283783, 0.3040052652359009]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=4)\n",
    "\n",
    "# Evaluating results with test set:\n",
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[683 158]\n",
      " [319 363]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.81      0.74       841\n",
      "           1       0.70      0.53      0.60       682\n",
      "\n",
      "    accuracy                           0.69      1523\n",
      "   macro avg       0.69      0.67      0.67      1523\n",
      "weighted avg       0.69      0.69      0.68      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_arg = np.argmax(y_test, axis=1)\n",
    "y_test_arg[1]\n",
    "y_pred = np.argmax(model.predict(X_test),axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test_arg, y_pred))\n",
    "print(metrics.classification_report(y_test_arg, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AHYcK_tICb3"
   },
   "source": [
    "# 7. Models after removing all the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "gH0-x3_OID8C"
   },
   "outputs": [],
   "source": [
    "df = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRieS82V3uxn"
   },
   "source": [
    "### Removing Punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NVrSUteG3iWi",
    "outputId": "806ce6b2-0e65-4939-efa0-0fa280f12e49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "IGkijhQ84Ycq"
   },
   "outputs": [],
   "source": [
    "punctuations_list = string.punctuation\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: cleaning_punctuations(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhQgMvaP31XO"
   },
   "source": [
    "### Removing Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Up4kO1Qw4bPE"
   },
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CGqcOfV367y"
   },
   "source": [
    "### Removing Numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "6VLYSul94dId"
   },
   "outputs": [],
   "source": [
    "def cleaning_numbers(text):\n",
    "    return re.sub('[0-9]+', '', text)\n",
    "\n",
    "df['text'] = df['text'].apply(lambda text: cleaning_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syTrKFQNCHtj"
   },
   "source": [
    "### Removing repeating characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "qYgP8wXzCKZ-"
   },
   "outputs": [],
   "source": [
    "tokens = (word_tokenize(i) for i in df.text)\n",
    "df['text'] = df['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "pattern = re.compile(r'(.)\\1*')\n",
    "\n",
    "def reduce_sequence_word(word):\n",
    "    return ''.join([match.group()[:2] if len(match.group()) > 2 else match.group() for match in pattern.finditer(word)])\n",
    "\n",
    "def reduce_sequence_tweet(tweet):\n",
    "    return [reduce_sequence_word(word) for word in tweet]\n",
    "\n",
    "df.text = df.text.apply(lambda tweet: reduce_sequence_tweet(tweet))\n",
    "\n",
    "# Detokenizing tweets:\n",
    "\n",
    "def listToString(s): \n",
    "    \n",
    "    # initialize an empty string\n",
    "    str1 = \" \" \n",
    "    \n",
    "    # return string  \n",
    "    return (str1.join(s))\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: listToString(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Stemming: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing tweets:\n",
    "tokens = (word_tokenize(i) for i in df.text)\n",
    "df['text'] = df['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "stemm = SnowballStemmer('english')\n",
    "df['text'] = df['text'].apply(lambda x: [stemm.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a3qKrLa4UP6"
   },
   "source": [
    "### Splitting data into Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "ggZvcwG0l6q7"
   },
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(df['target'].astype(str), num_classes=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'].astype(str), y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uf_4sAyuJLIq"
   },
   "source": [
    "### Training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJTri_ZlhGA4",
    "outputId": "08e429e1-343f-4a3d-ea68-e3596ab0cb70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "191/191 [==============================] - 241s 1s/step - loss: 0.6524 - accuracy: 0.7015 - precision: 0.6297 - recall: 0.3591\n",
      "Epoch 2/4\n",
      "191/191 [==============================] - 238s 1s/step - loss: 0.6465 - accuracy: 0.6964 - precision: 0.6358 - recall: 0.3609\n",
      "Epoch 3/4\n",
      "191/191 [==============================] - 237s 1s/step - loss: 0.6373 - accuracy: 0.6957 - precision: 0.6439 - recall: 0.3637\n",
      "Epoch 4/4\n",
      "191/191 [==============================] - 236s 1s/step - loss: 0.6325 - accuracy: 0.6880 - precision: 0.6554 - recall: 0.3813\n",
      "48/48 [==============================] - 59s 1s/step - loss: 0.6318 - accuracy: 0.6800 - precision: 0.7013 - recall: 0.3145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6317766904830933,\n",
       " 0.6799976229667664,\n",
       " 0.7013177275657654,\n",
       " 0.3145108222961426]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=4)\n",
    "\n",
    "# Evaluating results with test set:\n",
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HqeR3Q8Ict7"
   },
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6WKfiGDNLWaN",
    "outputId": "b75cb037-3fb1-4a8e-8f43-fc08a066ba03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[393 448]\n",
      " [109 573]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.47      0.59       841\n",
      "           1       0.56      0.84      0.67       682\n",
      "\n",
      "    accuracy                           0.63      1523\n",
      "   macro avg       0.67      0.65      0.63      1523\n",
      "weighted avg       0.68      0.63      0.62      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_arg = np.argmax(y_test, axis=1)\n",
    "y_test_arg[1]\n",
    "y_pred = np.argmax(model.predict(X_test),axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test_arg, y_pred))\n",
    "print(metrics.classification_report(y_test_arg, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwAqjLaPAPy7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT - Research.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
