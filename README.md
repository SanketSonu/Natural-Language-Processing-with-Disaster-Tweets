# Natural-Language-Processing-with-Disaster-Tweets
This is a Kaggle Competition dataset: https://www.kaggle.com/competitions/nlp-getting-started
Kaggle Notebook: https://www.kaggle.com/code/sanketsonu/ml-models-with-multiple-setups

These Machine Learning models are trained for each setup. 
After training and testing the best model and setups will be used to train the machine learning models finally a submission file was created and I was able to score 80.29% accuracy on the unlabelled test set and was ranked 261st on the leaderboard.
I will create a few more deep learning models like LSTM, and CNN and will try BERT as well, which I will upload later on.
